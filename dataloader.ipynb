{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d0355b",
   "metadata": {},
   "source": [
    "related lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc77b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "\n",
    "# 下载必要的NLTK资源\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet', quiet=True)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 设置matplotlib中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n",
    "\n",
    "def clear_huggingface_cache():\n",
    "    \"\"\"\n",
    "    Clear HuggingFace dataset cache\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cache_dir = os.path.expanduser(\"~/.cache/huggingface/datasets\")\n",
    "        if os.path.exists(cache_dir):\n",
    "            shutil.rmtree(cache_dir)\n",
    "            print(f\"HuggingFace cache directory cleared: {cache_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning clearing cache: {e}\")\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean text data\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    # 移除HTML标签\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # 移除特殊字符和数字\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # 转换为小写\n",
    "    text = text.lower()\n",
    "    # 去除多余空格\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_text(text: str, remove_stopwords: bool = True, lemmatize: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Advanced text preprocessing\n",
    "    \n",
    "    Args:\n",
    "        text: Original text\n",
    "        remove_stopwords: Whether to remove stopwords\n",
    "        lemmatize: Whether to lemmatize words\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed text\n",
    "    \"\"\"\n",
    "    # 基础清理\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # 分词\n",
    "    words = text.split()\n",
    "    \n",
    "    # 移除停用词\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # 词形还原\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "def get_tokenizer(model_name='bert-base-uncased'):\n",
    "    \"\"\"\n",
    "    Get pre-trained tokenizer\n",
    "    \n",
    "    Args:\n",
    "        model_name: Pre-trained model name\n",
    "        \n",
    "    Returns:\n",
    "        Tokenizer instance\n",
    "    \"\"\"\n",
    "    return BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def load_yelp_dataset():\n",
    "    \"\"\"\n",
    "    Load Yelp dataset from Hugging Face\n",
    "    \n",
    "    Args:\n",
    "        save_to_local: Whether to save to local CSV\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: (train_df, test_df)\n",
    "    \"\"\"\n",
    "    print(\"=== Yelp Dataset Loader ===\")\n",
    "    print(\"Loading yelp_review_full from HuggingFace...\")\n",
    "    clear_huggingface_cache()\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"yelp_review_full\", cache_dir=None)\n",
    "        \n",
    "        # Convert to DataFrames\n",
    "        train_df = pd.DataFrame({'text': dataset['train']['text'], 'label': dataset['train']['label']})\n",
    "        test_df = pd.DataFrame({'text': dataset['test']['text'], 'label': dataset['test']['label']})\n",
    "        \n",
    "        # Convert labels from 0-4 to 1-5 stars\n",
    "        train_df['label'] += 1\n",
    "        test_df['label'] += 1\n",
    "        \n",
    "        print(f\"Loaded {len(train_df)} train samples and {len(test_df)} test samples\")\n",
    "        print(f\"Train label distribution: {train_df['label'].value_counts().sort_index().to_dict()}\")\n",
    "        \n",
    "        return train_df, test_df\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load from HuggingFace: {e}\")\n",
    "        raise\n",
    "\n",
    "def preprocess_data(train_df: pd.DataFrame, test_df: pd.DataFrame, \n",
    "                   remove_stopwords: bool = True, lemmatize: bool = True, \n",
    "                   batch_size: Optional[int] = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Preprocess text data with advanced options\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training DataFrame\n",
    "        test_df: Test DataFrame\n",
    "        remove_stopwords: Whether to remove stopwords\n",
    "        lemmatize: Whether to lemmatize words\n",
    "        batch_size: Batch size for processing (None means process all at once)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: Preprocessed train and test DataFrames\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing text data...\")\n",
    "    \n",
    "    # 复制数据\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    # 高级文本预处理\n",
    "    preprocess_func = lambda text: preprocess_text(text, remove_stopwords, lemmatize)\n",
    "    \n",
    "    # 分批处理以减少内存使用\n",
    "    if batch_size and batch_size > 0:\n",
    "        print(f\"Processing in batches of {batch_size}...\")\n",
    "        \n",
    "        # 处理训练集\n",
    "        processed_train_texts = []\n",
    "        for i in range(0, len(train_df), batch_size):\n",
    "            batch_texts = train_df['text'].iloc[i:i+batch_size].apply(preprocess_func)\n",
    "            processed_train_texts.extend(batch_texts.tolist())\n",
    "            print(f\"  Processed {min(i+batch_size, len(train_df))}/{len(train_df)} train samples\")\n",
    "        train_df['text'] = processed_train_texts\n",
    "        \n",
    "        # 处理测试集\n",
    "        processed_test_texts = []\n",
    "        for i in range(0, len(test_df), batch_size):\n",
    "            batch_texts = test_df['text'].iloc[i:i+batch_size].apply(preprocess_func)\n",
    "            processed_test_texts.extend(batch_texts.tolist())\n",
    "            print(f\"  Processed {min(i+batch_size, len(test_df))}/{len(test_df)} test samples\")\n",
    "        test_df['text'] = processed_test_texts\n",
    "    else:\n",
    "        # 一次性处理\n",
    "        train_df['text'] = train_df['text'].apply(preprocess_func)\n",
    "        test_df['text'] = test_df['text'].apply(preprocess_func)\n",
    "    \n",
    "    # 添加文本长度特征\n",
    "    train_df['text_length'] = train_df['text'].str.len()\n",
    "    test_df['text_length'] = test_df['text'].str.len()\n",
    "    \n",
    "    # 过滤空文本\n",
    "    train_df = train_df[train_df['text'].str.len() > 0].reset_index(drop=True)\n",
    "    test_df = test_df[test_df['text'].str.len() > 0].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Preprocessed: {len(train_df)} train, {len(test_df)} test samples\")\n",
    "    return train_df, test_df\n",
    "\n",
    "def analyze_data(df: pd.DataFrame, data_name: str = \"Dataset\") -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze the dataset and return statistics\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "        data_name: Name of the dataset\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Analysis results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== {data_name} Analysis ===\")\n",
    "    \n",
    "    # 基本统计\n",
    "    total_samples = len(df)\n",
    "    print(f\"Total samples: {total_samples}\")\n",
    "    \n",
    "    # 标签分布\n",
    "    label_counts = df['label'].value_counts().sort_index()\n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    for label, count in label_counts.items():\n",
    "        percentage = (count / total_samples) * 100\n",
    "        print(f\"  {label} stars: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # 文本长度统计\n",
    "    min_length = df['text_length'].min()\n",
    "    max_length = df['text_length'].max()\n",
    "    avg_length = df['text_length'].mean()\n",
    "    median_length = df['text_length'].median()\n",
    "    \n",
    "    print(f\"\\nText length statistics:\")\n",
    "    print(f\"  Minimum: {min_length} characters\")\n",
    "    print(f\"  Maximum: {max_length} characters\")\n",
    "    print(f\"  Average: {avg_length:.2f} characters\")\n",
    "    print(f\"  Median: {median_length} characters\")\n",
    "    \n",
    "    # 词汇统计\n",
    "    total_words = df['text'].str.split().str.len().sum()\n",
    "    avg_words = total_words / total_samples\n",
    "    print(f\"\\nVocabulary statistics:\")\n",
    "    print(f\"  Total words: {total_words}\")\n",
    "    print(f\"  Average words per sample: {avg_words:.2f}\")\n",
    "    \n",
    "    # 计算不重复词汇数\n",
    "    unique_words = set()\n",
    "    batch_size = 10000  # 每次处理的样本数\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_text = ' '.join(df['text'].iloc[i:i+batch_size].tolist())\n",
    "        batch_words = batch_text.split()\n",
    "        unique_words.update(batch_words)\n",
    "    \n",
    "    unique_words_count = len(unique_words)\n",
    "    print(f\"  Unique words: {unique_words_count}\")\n",
    "    \n",
    "    # 返回分析结果\n",
    "    return {\n",
    "        'total_samples': total_samples,\n",
    "        'label_counts': label_counts.to_dict(),\n",
    "        'text_length': {\n",
    "            'min': min_length,\n",
    "            'max': max_length,\n",
    "            'avg': avg_length,\n",
    "            'median': median_length\n",
    "        },\n",
    "        'vocabulary': {\n",
    "            'total_words': total_words,\n",
    "            'avg_words_per_sample': avg_words,\n",
    "            'unique_words': unique_words_count\n",
    "        }\n",
    "    }\n",
    "\n",
    "def visualize_data(df: pd.DataFrame, output_dir: str = \"analysis_results\", sample_size: int = 100000) -> None:\n",
    "    \"\"\"\n",
    "    Visualize dataset characteristics with memory optimization\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to visualize\n",
    "        output_dir: Directory to save visualizations\n",
    "        sample_size: Maximum number of samples to use for visualization\n",
    "    \"\"\"\n",
    "    # 创建输出目录\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 对大型数据集进行采样以减少内存使用\n",
    "    if len(df) > sample_size:\n",
    "        df_sample = df.sample(n=sample_size, random_state=42)\n",
    "        print(f\"Using sample of {sample_size} out of {len(df)} samples for visualization\")\n",
    "    else:\n",
    "        df_sample = df\n",
    "    \n",
    "    # 1. 标签分布 - 使用原始数据确保准确计数\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    label_counts = df['label'].value_counts().sort_index()\n",
    "    label_counts.plot(kind='bar', color=sns.color_palette('viridis', len(label_counts)))\n",
    "    plt.title('Label Distribution (Star Ratings)')\n",
    "    plt.xlabel('Star Rating')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'label_distribution.png'), dpi=100)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. 文本长度分布\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df_sample['text_length'], bins=50, kde=True, color='blue')\n",
    "    plt.title('Text Length Distribution')\n",
    "    plt.xlabel('Text Length (characters)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'text_length_distribution.png'), dpi=100)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. 文本长度与标签的关系\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='label', y='text_length', data=df_sample, palette='viridis')\n",
    "    plt.title('Text Length vs Star Rating')\n",
    "    plt.xlabel('Star Rating')\n",
    "    plt.ylabel('Text Length (characters)')\n",
    "    \n",
    "    # 去除异常值以提高可读性\n",
    "    max_length = df_sample['text_length'].quantile(0.95)\n",
    "    plt.ylim(0, max_length)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'text_length_vs_rating.png'), dpi=100)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Visualizations saved to {output_dir}\")\n",
    "\n",
    "def test_preprocessing():\n",
    "    \"\"\"Test data loading, preprocessing and analysis\"\"\"\n",
    "    print(\"=== Testing Yelp Dataset Loader ===\")\n",
    "    \n",
    "    try:\n",
    "        # Load dataset\n",
    "        train_df, test_df = load_yelp_dataset()\n",
    "        print(f\"✓ Dataset loaded successfully: {len(train_df)} train, {len(test_df)} test samples\")\n",
    "        print(f\"  Columns: {list(train_df.columns)}\")\n",
    "        \n",
    "        # Advanced preprocessing with memory optimization\n",
    "        train_processed, test_processed = preprocess_data(\n",
    "            train_df, test_df, \n",
    "            remove_stopwords=True, \n",
    "            lemmatize=True,\n",
    "            batch_size=50000  # 使用分批处理减少内存占用\n",
    "        )\n",
    "        print(f\"✓ Data preprocessed successfully: {len(train_processed)} train, {len(test_processed)} test samples\")\n",
    "        \n",
    "        # Analyze data\n",
    "        print(\"\\n=== Analyzing Preprocessed Data ===\")\n",
    "        train_analysis = analyze_data(train_processed, \"Training Set\")\n",
    "        test_analysis = analyze_data(test_processed, \"Test Set\")\n",
    "        \n",
    "        # Visualize data with sampling\n",
    "        print(\"\\n=== Generating Visualizations ===\")\n",
    "        combined_df = pd.concat([train_processed, test_processed], keys=['train', 'test']).reset_index(level=0).rename(columns={'level_0': 'set'})\n",
    "        visualize_data(combined_df, sample_size=100000)  # 使用采样减少内存使用\n",
    "        \n",
    "        print(f\"\\n✓ All tests completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(\"\\n=== Testing Complete ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_preprocessing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
